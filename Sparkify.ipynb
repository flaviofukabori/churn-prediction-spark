{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Predition - Sparkify Project\n",
    "This project build a classification model to predict user churn analysing user activities on a music streaming app called Sparkify.\n",
    "\n",
    "The project is build using Pyspark and pass through the steps below:\n",
    "\n",
    "- Load data to SparkSession\n",
    "- Clean and Preprocess Data\n",
    "- Exploratory Data Analysis - EDA\n",
    "- Feature Selection\n",
    "- Model building and optimization\n",
    "- Model evalution\n",
    "\n",
    "The analysis is initialy done on a tiny subset (128MB) of the full dataset available (12GB). \n",
    "\n",
    "Once the initial analysis is done, the full pipeline with all the dataset (12GB) is processed on AWS EMR Spark Cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf, lag\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkifly Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "\n",
    "file = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows: 286500\n"
     ]
    }
   ],
   "source": [
    "print('total number of rows:' , df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30'),\n",
       " Row(artist='Five Iron Frenzy', auth='Logged In', firstName='Micah', gender='M', itemInSession=79, lastName='Long', length=236.09424, level='free', location='Boston-Cambridge-Newton, MA-NH', method='PUT', page='NextSong', registration=1538331630000, sessionId=8, song='Canada', status=200, ts=1538352180000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.103 Safari/537.36\"', userId='9')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean rows with empty userId\n",
    "df = df.filter(\"userId <> ''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct users:  225\n"
     ]
    }
   ],
   "source": [
    "# number of distinct users\n",
    "n_users = df.select(\"userId\").distinct().count()\n",
    "\n",
    "print(\"number of distinct users: \", n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type of subscription\n",
    "df.select(\"level\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|page                     |\n",
      "+-------------------------+\n",
      "|Cancel                   |\n",
      "|Submit Downgrade         |\n",
      "|Thumbs Down              |\n",
      "|Home                     |\n",
      "|Downgrade                |\n",
      "|Roll Advert              |\n",
      "|Logout                   |\n",
      "|Save Settings            |\n",
      "|Cancellation Confirmation|\n",
      "|About                    |\n",
      "|Settings                 |\n",
      "|Add to Playlist          |\n",
      "|Add Friend               |\n",
      "|NextSong                 |\n",
      "|Thumbs Up                |\n",
      "|Help                     |\n",
      "|Upgrade                  |\n",
      "|Error                    |\n",
      "|Submit Upgrade           |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type of page that user visit\n",
    "df.select(\"page\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------------------+\n",
      "|userId|       page|              artist|\n",
      "+------+-----------+--------------------+\n",
      "|100010|   NextSong|Sleeping With Sirens|\n",
      "|100010|   NextSong|Francesca Battist...|\n",
      "|100010|   NextSong|              Brutha|\n",
      "|100010|  Thumbs Up|                null|\n",
      "|100010|   NextSong|         Josh Ritter|\n",
      "|100010|   NextSong|               LMFAO|\n",
      "|100010|   NextSong|         OneRepublic|\n",
      "|100010|   NextSong|       Dwight Yoakam|\n",
      "|100010|Roll Advert|                null|\n",
      "|100010|   NextSong|      The Chordettes|\n",
      "|100010|   NextSong|Coko featuring Ki...|\n",
      "|100010|   NextSong|            The Cure|\n",
      "|100010|Roll Advert|                null|\n",
      "|100010|   NextSong|Kid Cudi Vs Crookers|\n",
      "|100010|  Thumbs Up|                null|\n",
      "|100010|   NextSong|            Yeasayer|\n",
      "|100010|   NextSong|             Ben Lee|\n",
      "|100010|Roll Advert|                null|\n",
      "|100010|   NextSong|  ? & The Mysterians|\n",
      "|100010|Roll Advert|                null|\n",
      "+------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"userId = '100010'\").select([\"userId\", \"page\", \"artist\"]).sort([\"ts\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of disctinct locations:  114\n"
     ]
    }
   ],
   "source": [
    "# user location\n",
    "print('number of disctinct locations: ',\n",
    "      df.select('location').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn\n",
    "\n",
    "A column `Churned` is created to identify users that has churned. \n",
    "`Cancellation Confirmation` events is used to define churn, which happen for both paid and free users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user churn\n",
    "hasCancelled = udf(lambda x: 1 if x=='Cancellation Confirmation' else 0, IntegerType())\n",
    "df = df.withColumn('Churn', hasCancelled(df['page']))\n",
    "\n",
    "user_window = Window.partitionBy('userId')\n",
    "df = df.withColumn('Churned', F.max('Churn').over(user_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate churn rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users:  225\n",
      "users churned:  52\n",
      "users not churned:  173\n",
      "churn rate:  0.2311111111111111\n"
     ]
    }
   ],
   "source": [
    "n_user_churn = df.select(['userId']).filter(\"Churn=1\").distinct().count()\n",
    "\n",
    "print(\"total users: \", n_users)\n",
    "print(\"users churned: \",n_user_churn)\n",
    "print(\"users not churned: \", (n_users - n_user_churn) )\n",
    "print(\"churn rate: \", n_user_churn/n_users )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data\n",
    "Once defined churn, we perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### info about the artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct artist:  17655\n",
      "distinct artist in churned users:  8402\n",
      "distinct artist not churned users:  16652\n",
      "artist only in churned:  1003\n",
      "artist only in not churned:  9253\n"
     ]
    }
   ],
   "source": [
    "artist_no_churned = np.array(df.filter('artist is not null and Churned=0').select('artist').collect())\n",
    "artist_no_churned = set(artist_no_churned.flatten())\n",
    "\n",
    "artist_churned = np.array(df.filter('artist is not null and Churned=1').select('artist').collect())\n",
    "artist_churned = set(artist_churned.flatten())\n",
    "\n",
    "artist_only_churn = artist_churned - artist_no_churned\n",
    "artist_only_no_churn = artist_no_churned - artist_churned\n",
    "\n",
    "print('distinct artist: ',\n",
    "      '17655')\n",
    "      #df.filter('artist is not null').select('artist').distinct().count())\n",
    "\n",
    "print('distinct artist in churned users: ',\n",
    "      len(artist_churned))\n",
    "\n",
    "print('distinct artist not churned users: ',\n",
    "      len(artist_no_churned))\n",
    "\n",
    "print('artist only in churned: ', \n",
    "      len(artist_churned - artist_no_churned))\n",
    "\n",
    "print('artist only in not churned: ',\n",
    "     len(artist_no_churned - artist_churned))\n",
    "\n",
    "#df = df.withColumn('artist_only_churn', col('artist').isin(artist_only_churn).cast('integer'))\n",
    "#df = df.withColumn('artist_only_no_churn', col('artist').isin(artist_only_no_churn).cast('integer')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users in group that has not churned listen songs from more distinct artists. Feature 'artist' will be included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "New features are created based on the original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column to count the number of songs played by the user\n",
    "is_song = udf(lambda x: 1 if x=='NextSong' else 0, IntegerType())\n",
    "df = df.withColumn('NextSong', is_song(df['page']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column to check if user has been in paid subscription\n",
    "has_paid= udf(lambda x: 1 if x=='paid' else 0, IntegerType()) \n",
    "df = df.withColumn('hasPaid', has_paid('level'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column to check if user has downgrade\n",
    "has_downgrade =  udf(lambda x: 1 if x=='Downgrade' else 0, IntegerType()) \n",
    "df = df.withColumn('hasDowngrade', has_downgrade('page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general info aggregated by userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general info aggregated by userId\n",
    "user = df.groupBy(\"userId\").agg(\n",
    "    F.max('location').alias('max_location'),\n",
    "    F.max('gender').alias('max_gender'),\n",
    "    F.max('hasPaid').alias('max_hasPaid'),\n",
    "    F.max('hasDowngrade').alias('max_hasDowngrade'),\n",
    "    F.max('Churn').alias('max_churn'),\n",
    "    F.countDistinct('artist').alias('dist_artist')\n",
    "#    F.collect_list(\"artist\").alias(\"artist_list\")\n",
    ")\n",
    "\n",
    "#user.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time between user sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time between sessions\n",
    "\n",
    "# rank in session\n",
    "session_by_ts_win = Window.partitionBy(['userId','sessionId']).orderBy('ts')\n",
    "df = df.withColumn('rank', F.rank().over(session_by_ts_win))\n",
    "\n",
    "# time_diff\n",
    "user_by_ts = Window.partitionBy(['userId']).orderBy('ts')\n",
    "df = df.withColumn('ts_diff', col('ts') - F.lag('ts',1).over(user_by_ts))\n",
    "\n",
    "# time diff between first page in current session - last page in previous session\n",
    "time_btw_session = df.select(['userId','ts_diff'])\\\n",
    "                         .filter(\"rank=1 and ts_diff is not null\")\\\n",
    "                         .groupBy('userId').agg(avg('ts_diff').alias('time_btw_sessions'))\n",
    "#time_btw_session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of songs played, and number os sessions aggregated BY user, session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of songs played, and number os sessions\n",
    "song_session = df.groupBy(['userId','sessionId'])\\\n",
    "                .agg(\n",
    "                    F.sum('length'), \n",
    "                    F.sum('NextSong')\\\n",
    "                )\\\n",
    "                .groupBy('userId')\\\n",
    "                .agg(\n",
    "                    F.avg('sum(length)').alias(\"avg_session_length\"),\n",
    "                    F.avg('sum(NextSong)').alias(\"avg_session_songs\"),\n",
    "                    F.sum('sum(length)').alias(\"total_session_length\"),   \n",
    "                    F.sum('sum(NextSong)').alias(\"total_songs_played\"),\n",
    "                    F.count('sum(length)').alias(\"total_sessions\")\n",
    "                )\n",
    "\n",
    "#song_session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join dataframes with aggregated columns\n",
    "\n",
    "This step joins 3 different dataframes that needed to be computed in separated steps:\n",
    "- data aggregated at the user level\n",
    "- data aggreated at the user and session level\n",
    "- time between session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = None\n",
    "joined_df = user.join(song_session, [\"userId\"], \"inner\")\\\n",
    "                .join(time_btw_session, [\"userId\"], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist processed dataframe and save to local file or Amazon S3\n",
    "To this point, all the preprecessing steps has been concluded. (cleaning, aggregation)\n",
    "\n",
    "Aggregated data will be stored to the local file system, or remote stored like Amazon S3.\n",
    "\n",
    "Thosee steps save computation time and resources, as all the followings steps (analysis, modelling) is done on aggregated info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2 = joined_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2.write.mode(\"overwrite\").parquet(\"joined_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse statistics using pandas\n",
    "\n",
    "The original small dataset has 286,500 rows. But after aggregating the original dataset to the userId level, we reduced the dataset to 210 rows.\n",
    "\n",
    "The full dataset has 26,259,199 rows. After aggregating, reduced to 22,278 rows.\n",
    "\n",
    "After aggregation, the analysis could be done using either spark dataframe or pandas dataframe. The following analysis is done using pandas dataframe as it easier to calculate stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pd = user2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features\n",
    "\n",
    "For numerical features, we compare the mean on users that has churned vs mean on users that has not churned. Features with more difference between those groups are good predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean for users in group not_churned and churned\n",
    "user_pd['avg_session_length'] = user_pd['avg_session_length']/(60*60)\n",
    "user_pd['total_session_length'] = user_pd['total_session_length']/(60*60)\n",
    "user_pd['time_btw_sessions'] = user_pd['time_btw_sessions']/(60*60*24*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_hasPaid</th>\n",
       "      <th>max_hasDowngrade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_churn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.719512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           max_hasPaid  max_hasDowngrade\n",
       "max_churn                               \n",
       "0             0.768293          0.719512\n",
       "1             0.760870          0.739130"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pd.groupby('max_churn')['max_hasPaid','max_hasDowngrade'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_artist</th>\n",
       "      <th>avg_session_length</th>\n",
       "      <th>avg_session_songs</th>\n",
       "      <th>total_session_length</th>\n",
       "      <th>total_songs_played</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>time_btw_sessions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_churn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>786.993902</td>\n",
       "      <td>5.036803</td>\n",
       "      <td>70.911949</td>\n",
       "      <td>80.635538</td>\n",
       "      <td>1164.829268</td>\n",
       "      <td>15.506098</td>\n",
       "      <td>6.026682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>581.456522</td>\n",
       "      <td>4.618036</td>\n",
       "      <td>64.868798</td>\n",
       "      <td>54.302568</td>\n",
       "      <td>786.173913</td>\n",
       "      <td>11.152174</td>\n",
       "      <td>2.894424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dist_artist  avg_session_length  avg_session_songs  \\\n",
       "max_churn                                                       \n",
       "0           786.993902            5.036803          70.911949   \n",
       "1           581.456522            4.618036          64.868798   \n",
       "\n",
       "           total_session_length  total_songs_played  total_sessions  \\\n",
       "max_churn                                                             \n",
       "0                     80.635538         1164.829268       15.506098   \n",
       "1                     54.302568          786.173913       11.152174   \n",
       "\n",
       "           time_btw_sessions  \n",
       "max_churn                     \n",
       "0                   6.026682  \n",
       "1                   2.894424  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pd.groupby('max_churn')['dist_artist', 'avg_session_length',\n",
    "       'avg_session_songs', 'total_session_length', 'total_songs_played',\n",
    "       'total_sessions', 'time_btw_sessions'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features with LOW difference on mean between not_churned and churned groups:**\n",
    "- max_hasPaid, max_hasDowngrade \n",
    "\n",
    "The stats show that users that at some point has paid or has downgrade do not make much diference on churn.\n",
    "\n",
    "Those features will NOT be used for modelling.\n",
    "\n",
    "**Features with HIGH difference on mean between not_churned and churned groups:**\n",
    "- dist_artist, \n",
    "- avg_session_length, \n",
    "- avg_session_songs, \n",
    "- total_session_length, \n",
    "- total_songs_played, \n",
    "- total_sessions, \n",
    "- time_btw_sessions\n",
    "\n",
    "Those features will be used for modelling\n",
    "\n",
    "Churned users tend to stay less time in each session and listen less songs according to (avg_session_length,\tavg_session_songs)\n",
    "\n",
    "Oddly, churned users tend connect more frequently than not churned users according to (time_btw_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoricals features\n",
    "\n",
    "Analysis of user gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>max_churn</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.758929</td>\n",
       "      <td>0.241071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "max_churn          0         1\n",
       "max_gender                    \n",
       "F           0.806122  0.193878\n",
       "M           0.758929  0.241071"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(user_pd['max_gender'], user_pd['max_churn']).apply(lambda x: x/x.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn rate seems to be higher for Males (24%) than Females (19%), so feature will be used for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "- Split the full dataset into train and test\n",
    "- Test out several of the machine learning (LogisticRegression, RandomForest, LinearSVC)\n",
    "- Evaluate the performance of various models, tuning parameters as necessary. \n",
    "- Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize.\n",
    "- Determine your winning model based on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading cleaned and aggregated data\n",
    "df2 = spark.read.parquet(\"joined_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- max_location: string (nullable = true)\n",
      " |-- max_gender: string (nullable = true)\n",
      " |-- max_hasPaid: integer (nullable = true)\n",
      " |-- max_hasDowngrade: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- dist_artist: long (nullable = true)\n",
      " |-- avg_session_length: double (nullable = true)\n",
      " |-- avg_session_songs: double (nullable = true)\n",
      " |-- total_session_length: double (nullable = true)\n",
      " |-- total_songs_played: long (nullable = true)\n",
      " |-- total_sessions: long (nullable = true)\n",
      " |-- time_btw_sessions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define label colum\n",
    "df2 = df2.withColumnRenamed('max_churn','label')\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into validation and test set\n",
    "\n",
    "- validation set is used for crossvalidation e gridsearch\n",
    "- test set is used to evaluate the model perfomance on 'unseen' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into validation and test set\n",
    "validation, test = df2.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to model training and model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, paramGrid):\n",
    "    \"\"\"\n",
    "    Build a pipeline and fit a model using CrossValidator and hyperparam optimization.\n",
    "    Numerical features are Scaled with StandardScaler\n",
    "    Categorical feature gender is transformed with StringIndexer\n",
    "    \n",
    "    Args:\n",
    "    model (spark.ml.classification): The classifier algorithm\n",
    "    paramGrid (ParamGridBuilder): A grid of hyperparameters used to optimize the model\n",
    "    \n",
    "    return:\n",
    "    fitted_model: A cross validated fitted model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #cv = CountVectorizer(inputCol=\"artist_list\", outputCol=\"TF\")\n",
    "    #location_index = StringIndexer(inputCol='max_location', outputCol=\"location_index\", handleInvalid='skip')\n",
    "\n",
    "    gender_index = StringIndexer(inputCol='max_gender', outputCol=\"gender_index\", handleInvalid='keep')\n",
    "\n",
    "    #scaler\n",
    "    #\"max_hasDowngrade\",\"max_hasPaid\",\n",
    "    numerical = [\"dist_artist\", \"avg_session_length\",\n",
    "                 \"avg_session_songs\",\"total_session_length\",\"total_songs_played\",\n",
    "                 \"total_sessions\",\"time_btw_sessions\"]\n",
    "\n",
    "    numfeatures = VectorAssembler(inputCols=numerical, outputCol='NumFeatures')\n",
    "    scaler = StandardScaler(inputCol='NumFeatures', outputCol=\"ScaledNumFeatures\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[#\"TF\",\n",
    "                                           #\"location_index\",\n",
    "                                           \"gender_index\",\n",
    "                                           \"ScaledNumFeatures\"\n",
    "                                          ], \n",
    "                                outputCol='features')\n",
    "\n",
    "    pipeline = Pipeline(stages=[gender_index,\n",
    "                                numfeatures, \n",
    "                                scaler, \n",
    "                                assembler,\n",
    "                                model])\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                              numFolds=3)\n",
    "\n",
    "    fitted_model = crossval.fit(validation)\n",
    "    \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "    \"\"\"\n",
    "    Evaluate the predited values VS real values\n",
    "    \n",
    "    Returns:\n",
    "    score (float): F1-Score\n",
    "    \"\"\"\n",
    "    score = MulticlassClassificationEvaluator(metricName='f1').evaluate(results)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test several models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Logistic Regression\n",
    "############################\n",
    "lr=  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "\n",
    "lr_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = fit_model(lr, lr_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8632573240997183, 0.6687792685395348]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest f1-score:  0.9\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_results(lr_model.transform(test))\n",
    "print('RandomForest f1-score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# RandomForestClassifier\n",
    "##########################\n",
    "rf = RandomForestClassifier()\n",
    "rf_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth,[4,10,20])\\\n",
    "    .addGrid(rf.numTrees,[15,30])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = fit_model(rf, rf_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.820194369551875,\n",
       " 0.7896157542673772,\n",
       " 0.7887644405621153,\n",
       " 0.8127544839528458,\n",
       " 0.7887644405621153,\n",
       " 0.8127544839528458]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest f1-score:  0.9423423423423425\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_results(rf_model.transform(test))\n",
    "print('RandomForest f1-score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# LinearSVC\n",
    "############\n",
    "lsvc = LinearSVC()\n",
    "lsvc_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lsvc.maxIter,[10,20])\\\n",
    "    .addGrid(lsvc.regParam,[0.0,0.1])\\\n",
    "    .build()\n",
    "\n",
    "lsvc_model = fit_model(lsvc, lsvc_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6687792685395348,\n",
       " 0.6687792685395348,\n",
       " 0.6766347046207773,\n",
       " 0.6687792685395348]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC f1-score:  0.8526315789473685\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_results(lsvc_model.transform(test))\n",
    "print('Linear SVC f1-score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## **Results on small dataset:**\n",
    "\n",
    "F1-Score on (cross-validation, test-set):\n",
    "- LogisticRegression = (0.863, 0.900)\n",
    "- RandomForest = (0.820, 0.942)\n",
    "- LinearSVC = (0.668, 0.852)\n",
    "\n",
    "RandomForest had the best performance on the small dataset, with F1-score = 0.942 on test-set. However, there was a high variance on metrics when cross-validation and test-set were compared.\n",
    "\n",
    "I suspected that the high variance was due to the small sample size on small dataset. To confirm that hypothesis, the same code was run on the full dataset at AWS EMR cluster..\n",
    "\n",
    "\n",
    "## **Results on full dataset, run on Amazon EMR Cluster:**\n",
    "\n",
    "The 3 models were run on the full dataset at Amazon EMR Cluster as well:\n",
    "F1-Score on (cross-validation, test-set)\n",
    "- LogisticRegression = (0.805, 0.799)\n",
    "- RandomForest = (0.879, 0.872)\n",
    "- LinearSVC = (0.688, 0.679)\n",
    "\n",
    "As expected, the variance between cross-validation and test-set score on the full dataset is almost zero. This confirm the hypothesis of high variance on the small data set is due to sample size.\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "LinearSVC had a poor perfomance on the full dataset\n",
    "\n",
    "RandomForest had the best perfomance on the full dataset, with F1-score = 0.872. \n",
    "\n",
    "Based on the results above, I suggest to select RandomForest to deploy in production, as it has presented the best performance on the full dataset and has presented the lowest difference between the results on small and full dataset.\n",
    "\n",
    "If computation costs or model explicability is a constraint, I would pick LogisticRegression to deploy in production.\n",
    "\n",
    "**Future works**\n",
    "\n",
    "Before a deployment in production, we could work on the followings steps:\n",
    "- Run a new training increasing the quantity of hyperparameters and numFolds\n",
    "- Run a new training with different algorithm, like boosting\n",
    "- Analyse precision vs recall metrics and optmize the threshold based on the business goals.  Prioritize precision if the cost of false positive is high, or prioritize recall if the cost of false negative is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}