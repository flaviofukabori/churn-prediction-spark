{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf, lag\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkifly Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "#event_data = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "file = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(file)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows: 286500\n"
     ]
    }
   ],
   "source": [
    "print('total number of rows:' , df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30'),\n",
       " Row(artist='Five Iron Frenzy', auth='Logged In', firstName='Micah', gender='M', itemInSession=79, lastName='Long', length=236.09424, level='free', location='Boston-Cambridge-Newton, MA-NH', method='PUT', page='NextSong', registration=1538331630000, sessionId=8, song='Canada', status=200, ts=1538352180000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.103 Safari/537.36\"', userId='9')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean rows with empty userId\n",
    "df = df.filter(\"userId <> ''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct users:  225\n"
     ]
    }
   ],
   "source": [
    "# number of distinct users\n",
    "n_users = df.select(\"userId\").distinct().count()\n",
    "\n",
    "print(\"number of distinct users: \", n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-08ee4ccde399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# type of subscription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# type of subscription\n",
    "df.select(\"level\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|page                     |\n",
      "+-------------------------+\n",
      "|Cancel                   |\n",
      "|Submit Downgrade         |\n",
      "|Thumbs Down              |\n",
      "|Home                     |\n",
      "|Downgrade                |\n",
      "|Roll Advert              |\n",
      "|Logout                   |\n",
      "|Save Settings            |\n",
      "|Cancellation Confirmation|\n",
      "|About                    |\n",
      "|Settings                 |\n",
      "|Add to Playlist          |\n",
      "|Add Friend               |\n",
      "|NextSong                 |\n",
      "|Thumbs Up                |\n",
      "|Help                     |\n",
      "|Upgrade                  |\n",
      "|Error                    |\n",
      "|Submit Upgrade           |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type of page that user visit\n",
    "df.select(\"page\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------------------+\n",
      "|userId|       page|              artist|\n",
      "+------+-----------+--------------------+\n",
      "|100010|   NextSong|Sleeping With Sirens|\n",
      "|100010|   NextSong|Francesca Battist...|\n",
      "|100010|   NextSong|              Brutha|\n",
      "|100010|  Thumbs Up|                null|\n",
      "|100010|   NextSong|         Josh Ritter|\n",
      "|100010|   NextSong|               LMFAO|\n",
      "|100010|   NextSong|         OneRepublic|\n",
      "|100010|   NextSong|       Dwight Yoakam|\n",
      "|100010|Roll Advert|                null|\n",
      "|100010|   NextSong|      The Chordettes|\n",
      "|100010|   NextSong|Coko featuring Ki...|\n",
      "|100010|   NextSong|            The Cure|\n",
      "|100010|Roll Advert|                null|\n",
      "|100010|   NextSong|Kid Cudi Vs Crookers|\n",
      "|100010|  Thumbs Up|                null|\n",
      "|100010|   NextSong|            Yeasayer|\n",
      "|100010|   NextSong|             Ben Lee|\n",
      "|100010|Roll Advert|                null|\n",
      "|100010|   NextSong|  ? & The Mysterians|\n",
      "|100010|Roll Advert|                null|\n",
      "+------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"userId = '100010'\").select([\"userId\", \"page\", \"artist\"]).sort([\"ts\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of disctinct locations:  114\n"
     ]
    }
   ],
   "source": [
    "# user location\n",
    "print('number of disctinct locations: ',\n",
    "      df.select('location').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define user  churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user churn\n",
    "hasCancelled = udf(lambda x: 1 if x=='Cancellation Confirmation' else 0, IntegerType())\n",
    "df = df.withColumn('Churn', hasCancelled(df['page']))\n",
    "\n",
    "user_window = Window.partitionBy('userId')\n",
    "df = df.withColumn('Churned', F.max('Churn').over(user_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|   125|\n",
      "|    51|\n",
      "|    54|\n",
      "|100014|\n",
      "|   101|\n",
      "|    29|\n",
      "|100021|\n",
      "|    87|\n",
      "|    73|\n",
      "|     3|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['userId']).filter(\"Churn=1\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate churn rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users:  225\n",
      "users churned:  52\n",
      "users not churned:  173\n",
      "churn rate:  0.2311111111111111\n"
     ]
    }
   ],
   "source": [
    "n_user_churn = df.select(['userId']).filter(\"Churn=1\").distinct().count()\n",
    "\n",
    "print(\"total users: \", n_users)\n",
    "print(\"users churned: \",n_user_churn)\n",
    "print(\"users not churned: \", (n_users - n_user_churn) )\n",
    "print(\"churn rate: \", n_user_churn/n_users )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column to count the number of songs played by the user\n",
    "is_song = udf(lambda x: 1 if x=='NextSong' else 0, IntegerType())\n",
    "df = df.withColumn('NextSong', is_song(df['page']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column to check if user has been in paid subscription\n",
    "has_paid= udf(lambda x: 1 if x=='paid' else 0, IntegerType()) \n",
    "df = df.withColumn('hasPaid', has_paid('level'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column to check if user has downgrade\n",
    "has_downgrade =  udf(lambda x: 1 if x=='Downgrade' else 0, IntegerType()) \n",
    "df = df.withColumn('hasDowngrade', has_downgrade('page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### info about the artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct artist:  17655\n",
      "distinct artist in churned users:  8402\n",
      "distinct artist not churned users:  16652\n",
      "artist only in churned:  1003\n",
      "artist only in not churned:  9253\n"
     ]
    }
   ],
   "source": [
    "artist_no_churned = np.array(df.filter('artist is not null and Churned=0').select('artist').collect())\n",
    "artist_no_churned = set(artist_no_churned.flatten())\n",
    "\n",
    "artist_churned = np.array(df.filter('artist is not null and Churned=1').select('artist').collect())\n",
    "artist_churned = set(artist_churned.flatten())\n",
    "\n",
    "artist_only_churn = artist_churned - artist_no_churned\n",
    "artist_only_no_churn = artist_no_churned - artist_churned\n",
    "\n",
    "print('distinct artist: ',\n",
    "      '17655')\n",
    "      #df.filter('artist is not null').select('artist').distinct().count())\n",
    "\n",
    "print('distinct artist in churned users: ',\n",
    "      len(artist_churned))\n",
    "\n",
    "print('distinct artist not churned users: ',\n",
    "      len(artist_no_churned))\n",
    "\n",
    "print('artist only in churned: ', \n",
    "      len(artist_churned - artist_no_churned))\n",
    "\n",
    "print('artist only in not churned: ',\n",
    "     len(artist_no_churned - artist_churned))\n",
    "\n",
    "#df = df.withColumn('artist_only_churn', col('artist').isin(artist_only_churn).cast('integer'))\n",
    "#df = df.withColumn('artist_only_no_churn', col('artist').isin(artist_only_no_churn).cast('integer')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "users in group that has not churned listen songs from more distinct artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general info aggregated by userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general info aggregated by userId\n",
    "user = df.groupBy(\"userId\").agg(\n",
    "    F.max('location').alias('max_location'),\n",
    "    F.max('gender').alias('max_gender'),\n",
    "    F.max('hasPaid').alias('max_hasPaid'),\n",
    "    F.max('hasDowngrade').alias('max_hasDowngrade'),\n",
    "    F.max('Churn').alias('max_churn'),\n",
    "    F.countDistinct('artist').alias('dist_artist')\n",
    "#    F.collect_list(\"artist\").alias(\"artist_list\")\n",
    ")\n",
    "\n",
    "#user.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time between user sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time between sessions\n",
    "\n",
    "# rank in session\n",
    "session_by_ts_win = Window.partitionBy(['userId','sessionId']).orderBy('ts')\n",
    "df = df.withColumn('rank', F.rank().over(session_by_ts_win))\n",
    "\n",
    "# time_diff\n",
    "user_by_ts = Window.partitionBy(['userId']).orderBy('ts')\n",
    "df = df.withColumn('ts_diff', col('ts') - F.lag('ts',1).over(user_by_ts))\n",
    "\n",
    "# time diff between first page in current session - last page in previous session\n",
    "time_btw_session = df.select(['userId','ts_diff'])\\\n",
    "                         .filter(\"rank=1 and ts_diff is not null\")\\\n",
    "                         .groupBy('userId').agg(avg('ts_diff').alias('time_btw_sessions'))\n",
    "#time_btw_session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of songs played, and number os sessions aggregated BY user, session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of songs played, and number os sessions\n",
    "song_session = df.groupBy(['userId','sessionId'])\\\n",
    "                .agg(\n",
    "                    F.sum('length'), \n",
    "                    F.sum('NextSong')\\\n",
    "                )\\\n",
    "                .groupBy('userId')\\\n",
    "                .agg(\n",
    "                    F.avg('sum(length)').alias(\"avg_session_length\"),\n",
    "                    F.avg('sum(NextSong)').alias(\"avg_session_songs\"),\n",
    "                    F.sum('sum(length)').alias(\"total_session_length\"),   \n",
    "                    F.sum('sum(NextSong)').alias(\"total_songs_played\"),\n",
    "                    F.count('sum(length)').alias(\"total_sessions\")\n",
    "                )\n",
    "\n",
    "#song_session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join dataframes with aggregated columns\n",
    "\n",
    "This step joins 3 different dataframes that needed to be computed in separated steps:\n",
    "- data aggregated at the user level\n",
    "- data aggreated at the user and session level\n",
    "- time between session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = None\n",
    "joined_df = user.join(song_session, [\"userId\"], \"inner\")\\\n",
    "                .join(time_btw_session, [\"userId\"], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist processed dataframe and save to local file or Amazon S3\n",
    "To this point, all the preprecessing steps has been concluded. (cleaning, aggregation)\n",
    "\n",
    "Aggregated data will be stored to the local file system, or remote stored like Amazon S3.\n",
    "\n",
    "Thosee steps save computation time and resources, as all the followings steps (analysis, modelling) is done on aggregated info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2 = joined_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2.write.mode(\"overwrite\").parquet(\"joined_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse statistics using pandas\n",
    "\n",
    "The original small dataset has 286,500 rows. But after aggregating the original dataset to the userId level, we reduced the dataset to 210 rows.\n",
    "\n",
    "The full dataset has 26,259,199 rows. After aggregating, reduced to 22,278 rows.\n",
    "\n",
    "After aggregation, the analysis could be done using either spark dataframe or pandas dataframe. The following analysis is done using pandas dataframe as it easier to calculate stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pd = user2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_hasPaid</th>\n",
       "      <th>max_hasDowngrade</th>\n",
       "      <th>dist_artist</th>\n",
       "      <th>avg_session_length</th>\n",
       "      <th>avg_session_songs</th>\n",
       "      <th>total_session_length</th>\n",
       "      <th>total_songs_played</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>time_btw_sessions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_churn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>786.993902</td>\n",
       "      <td>5.036803</td>\n",
       "      <td>70.911949</td>\n",
       "      <td>80.635538</td>\n",
       "      <td>1164.829268</td>\n",
       "      <td>15.506098</td>\n",
       "      <td>6.026682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>581.456522</td>\n",
       "      <td>4.618036</td>\n",
       "      <td>64.868798</td>\n",
       "      <td>54.302568</td>\n",
       "      <td>786.173913</td>\n",
       "      <td>11.152174</td>\n",
       "      <td>2.894424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           max_hasPaid  max_hasDowngrade  dist_artist  avg_session_length  \\\n",
       "max_churn                                                                   \n",
       "0             0.768293          0.719512   786.993902            5.036803   \n",
       "1             0.760870          0.739130   581.456522            4.618036   \n",
       "\n",
       "           avg_session_songs  total_session_length  total_songs_played  \\\n",
       "max_churn                                                                \n",
       "0                  70.911949             80.635538         1164.829268   \n",
       "1                  64.868798             54.302568          786.173913   \n",
       "\n",
       "           total_sessions  time_btw_sessions  \n",
       "max_churn                                     \n",
       "0               15.506098           6.026682  \n",
       "1               11.152174           2.894424  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean for users in group not_churned and churned\n",
    "user_pd['avg_session_length'] = user_pd['avg_session_length']/(60*60)\n",
    "user_pd['total_session_length'] = user_pd['total_session_length']/(60*60)\n",
    "user_pd['time_btw_sessions'] = user_pd['time_btw_sessions']/(60*60*24*1000)\n",
    "\n",
    "user_pd.groupby('max_churn').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features with LOW difference on mean between not_churned and churned groups:**\n",
    "- max_hasPaid, max_hasDowngrade \n",
    "\n",
    "The stats show that users that at some point has paid or has downgrade do not make much diference on churn.\n",
    "\n",
    "Those features will NOT be used for modelling.\n",
    "\n",
    "**Features with HIGH difference on mean between not_churned and churned groups:**\n",
    "- dist_artist, \n",
    "- avg_session_length, \n",
    "- avg_session_songs, \n",
    "- total_session_length, \n",
    "- total_songs_played, \n",
    "- total_sessions, \n",
    "- time_btw_sessions\n",
    "\n",
    "Those features will be used for modelling\n",
    "\n",
    "Churned users tend to stay less time in each session and listen less songs according to (avg_session_length,\tavg_session_songs)\n",
    "\n",
    "Oddly, churned users tend connect more frequently than not churned users according to (time_btw_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoricals features\n",
    "\n",
    "Analysis of user gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>max_churn</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.758929</td>\n",
       "      <td>0.241071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "max_churn          0         1\n",
       "max_gender                    \n",
       "F           0.806122  0.193878\n",
       "M           0.758929  0.241071"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(user_pd['max_gender'], user_pd['max_churn']).apply(lambda x: x/x.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn rate seems to be higher for Males (24%) than Females (19%), so feature will be used for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "- Split the full dataset into train and test\n",
    "- Test out several of the machine learning (LogisticRegression, RandomForest, LinearSVC)\n",
    "- Evaluate the performance of various models, tuning parameters as necessary. \n",
    "- Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize.\n",
    "- Determine your winning model based on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading cleaned and aggregated data\n",
    "df2 = spark.read.parquet(\"joined_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- max_location: string (nullable = true)\n",
      " |-- max_gender: string (nullable = true)\n",
      " |-- max_hasPaid: integer (nullable = true)\n",
      " |-- max_hasDowngrade: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- dist_artist: long (nullable = true)\n",
      " |-- avg_session_length: double (nullable = true)\n",
      " |-- avg_session_songs: double (nullable = true)\n",
      " |-- total_session_length: double (nullable = true)\n",
      " |-- total_songs_played: long (nullable = true)\n",
      " |-- total_sessions: long (nullable = true)\n",
      " |-- time_btw_sessions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define label colum\n",
    "df2 = df2.withColumnRenamed('max_churn','label')\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into validation and test set\n",
    "\n",
    "- validation set is used for crossvalidation e gridsearch\n",
    "- test set is used to evaluate the model perfomance on 'unseen' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into validation and test set\n",
    "validation, test = df2.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to model training and model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, paramGrid):\n",
    "    \"\"\"\n",
    "    Build a pipeline and fit a model using CrossValidator and hyperparam optimization.\n",
    "    Numerical features are Scaled with StandardScaler\n",
    "    Categorical feature gender is transformed with StringIndexer\n",
    "    \n",
    "    Args:\n",
    "    model (spark.ml.classification): The classifier algorithm\n",
    "    paramGrid (ParamGridBuilder): A grid of hyperparameters used to optimize the model\n",
    "    \n",
    "    return:\n",
    "    fitted_model: A cross validated fitted model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #cv = CountVectorizer(inputCol=\"artist_list\", outputCol=\"TF\")\n",
    "    #location_index = StringIndexer(inputCol='max_location', outputCol=\"location_index\", handleInvalid='skip')\n",
    "\n",
    "    gender_index = StringIndexer(inputCol='max_gender', outputCol=\"gender_index\", handleInvalid='keep')\n",
    "\n",
    "    #scaler\n",
    "    #\"max_hasDowngrade\",\"max_hasPaid\",\n",
    "    numerical = [\"dist_artist\", \"avg_session_length\",\n",
    "                 \"avg_session_songs\",\"total_session_length\",\"total_songs_played\",\n",
    "                 \"total_sessions\",\"time_btw_sessions\"]\n",
    "\n",
    "    numfeatures = VectorAssembler(inputCols=numerical, outputCol='NumFeatures')\n",
    "    scaler = StandardScaler(inputCol='NumFeatures', outputCol=\"ScaledNumFeatures\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[#\"TF\",\n",
    "                                           #\"location_index\",\n",
    "                                           \"gender_index\",\n",
    "                                           \"ScaledNumFeatures\"\n",
    "                                          ], \n",
    "                                outputCol='features')\n",
    "\n",
    "    pipeline = Pipeline(stages=[gender_index,\n",
    "                                numfeatures, \n",
    "                                scaler, \n",
    "                                assembler,\n",
    "                                model])\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                              numFolds=3)\n",
    "\n",
    "    fitted_model = crossval.fit(validation)\n",
    "    \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "    \"\"\"\n",
    "    Evaluate the predited values VS real values\n",
    "    \n",
    "    Returns:\n",
    "    score (float): F1-Score\n",
    "    \"\"\"\n",
    "    score = MulticlassClassificationEvaluator(metricName='f1').evaluate(results)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test several models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Logistic Regression\n",
    "############################\n",
    "lr=  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)\n",
    "\n",
    "lr_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = fit_model(lr, lr_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8326583420788993, 0.6785281750633183]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression f1-score:  0.9\n"
     ]
    }
   ],
   "source": [
    "test_results = model1.transform(test)\n",
    "score = evaluate_results(model1.transform(test))\n",
    "\n",
    "print('LogisticRegression f1-score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# RandomForestClassifier\n",
    "##########################\n",
    "rf = RandomForestClassifier()\n",
    "rf_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth,[4,10,20])\\\n",
    "    .addGrid(rf.numTrees,[15,30])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = fit_model(rf, rf_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8082284789073787,\n",
       " 0.8105364781673958,\n",
       " 0.8065433950052472,\n",
       " 0.8240975328771643,\n",
       " 0.8025585617028657,\n",
       " 0.8240975328771643]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest f1-score:  0.9423423423423425\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_results(model2.transform(test))\n",
    "print('RandomForest f1-score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC f1-score:  0.8526315789473685\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# LinearSVC\n",
    "############\n",
    "lsvc = LinearSVC()\n",
    "lsvc_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lsvc.maxIter,[10,20])\\\n",
    "    .addGrid(lsvc.regParam,[0.0,0.1])\\\n",
    "    .build()\n",
    "\n",
    "model3 = fit_model(lsvc, lsvc_grid)\n",
    "\n",
    "score = evaluate_results(model3.transform(test))\n",
    "\n",
    "print('Linear SVC f1-score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6673276801369108,\n",
       " 0.6673276801369108,\n",
       " 0.6673276801369108,\n",
       " 0.6673276801369108]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## **Results on small dataset:**\n",
    "\n",
    "F1-Score\n",
    "- LogisticRegression = 0.900\n",
    "- RandomForest = 0.942\n",
    "- LinearSVC = 0.852\n",
    "\n",
    "RandomForest had the best performance on the small dataset, with F1-score = 0.862\n",
    "\n",
    "The 3 models were run on the full dataset at Amazon EMR Cluster as well.\n",
    "\n",
    "\n",
    "## **Results on full dataset, run on Amazon EMR Cluster:**\n",
    "\n",
    "F1-Score\n",
    "- LogisticRegression = 0.8051\n",
    "- RandomForest = 0.879\n",
    "- LinearSVC = 0.679\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "LinearSVC had a poor perfomance on the full dataset\n",
    "\n",
    "RandomForest had the best perfomance on the full dataset, with F1-score = 0.879. \n",
    "\n",
    "Based on the results above, I suggest to select RandomForest to deploy in production, as it has presented the best performance on the full dataset and has presented the lowest difference between the results on small and full dataset.\n",
    "\n",
    "If computation costs or model explicability is a constraint, I would pick LogisticRegression to deploy in production.\n",
    "\n",
    "**Future works**\n",
    "\n",
    "Before a deployment in production, we could work on the followings steps:\n",
    "- Run a new training increasing the quantity of hyperparameters and numFolds\n",
    "- Run a new training with different algorithm, like boosting\n",
    "- Analyse precision vs recall metrics and optmize the threshold based on the business goals.  Prioritize precision if the cost of false positive is high, or prioritize recall if the cost of false negative is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
